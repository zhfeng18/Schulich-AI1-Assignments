{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Tweepy\" data-toc-modified-id=\"Import-Tweepy-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Tweepy</a></span></li><li><span><a href=\"#Analyze-Sentiments-Using-Twitter-Data-and-Tweepy-in-Python\" data-toc-modified-id=\"Analyze-Sentiments-Using-Twitter-Data-and-Tweepy-in-Python-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Analyze Sentiments Using Twitter Data and Tweepy in Python</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tweepy\n",
    "1. Import the tweepy package\n",
    "2. Set the authentication credentials\n",
    "3. Create a new tweepy.API object\n",
    "4. Use the api object to call the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T19:36:32.344626Z",
     "start_time": "2019-12-03T19:36:15.803934Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T19:50:42.427824Z",
     "start_time": "2019-12-03T19:36:36.325555Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import sst_binary, lr_results\n",
    "from encoder import Model\n",
    "\n",
    "model = Model()\n",
    "\n",
    "trX, vaX, teX, trY, vaY, teY = sst_binary()\n",
    "trXt = model.transform(trX[:10])\n",
    "vaXt = model.transform(vaX[:10])\n",
    "teXt = model.transform(teX[:10])\n",
    "\n",
    "clf = lr_results(trXt, trY[:10], vaXt, vaY[:10], teXt, teY[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T19:39:34.957315Z",
     "start_time": "2019-11-18T19:39:30.421448Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from utils import sst_binary, lr_results\n",
    "# from encoder import Model\n",
    "\n",
    "# model = Model()\n",
    "\n",
    "# trX, vaX, teX, trY, vaY, teY = sst_binary()\n",
    "# trXt = model.transform(trX)\n",
    "# vaXt = model.transform(vaX)\n",
    "# teXt = model.transform(teX)\n",
    "\n",
    "# clf = lr_results(trXt, trY, vaXt, vaY, teXt, teY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:03:17.018878Z",
     "start_time": "2019-11-18T22:03:16.994939Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(clf, 'logregress_clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T20:14:28.340984Z",
     "start_time": "2019-12-03T20:13:30.913447Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from encoder import Model\n",
    "\n",
    "clf = load('logregress_clf.joblib')\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T20:14:32.708027Z",
     "start_time": "2019-12-03T20:14:32.232103Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy as tw\n",
    "import re\n",
    "\n",
    "# Authenticate to Twitter\n",
    "consumer_key = \"1KbEaA98iLFrLI68cWwZBojKa\"\n",
    "consumer_secret = \"UMuEh1JiSu8mVtOpWnDKatg2tXCEf3RGrOyUNBPfC0NgPgUVz0\"\n",
    "access_token = \"786958989347033088-k12TcKdTNLTcr16b9W2eAioz07eAwu1\"\n",
    "access_token_secret = \"3sU3lCoT6b6Y1Gf1DZPENCaYbbd5B18Dkp8iSYNuLWCSV\"\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth)\n",
    "\n",
    "tag = \"@HarvardBiz\"\n",
    "\n",
    "tweets = api.search(q=tag, lang=\"en\", count=10)\n",
    "# tweets_text = [tweet.text for tweet in tweets]\n",
    "\n",
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "clean_tweets = [remove_url(tweet.text) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T20:14:33.472021Z",
     "start_time": "2019-12-03T20:14:33.460077Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T20:14:56.823195Z",
     "start_time": "2019-12-03T20:14:36.131389Z"
    }
   },
   "outputs": [],
   "source": [
    "tw_transfrom = model.transform(clean_tweets)\n",
    "predictions = clf.predict(tw_transfrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T20:14:58.487939Z",
     "start_time": "2019-12-03T20:14:58.479958Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T20:15:00.269205Z",
     "start_time": "2019-12-03T20:15:00.187765Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Search tag:\", tag)\n",
    "print()\n",
    "for i in range(0, 10):\n",
    "    tw1 = clean_tweets[i]\n",
    "    sa = predictions[i]\n",
    "    print(sa, tw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T20:40:29.712432Z",
     "start_time": "2019-11-18T20:40:29.390292Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets = api.user_timeline(tag, count=10)\n",
    "for tweet in tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T20:40:32.026340Z",
     "start_time": "2019-11-18T20:40:32.021352Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets1 = [tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T19:49:03.861996Z",
     "start_time": "2019-11-18T19:49:03.847035Z"
    }
   },
   "outputs": [],
   "source": [
    "type(tweets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T02:59:32.416144Z",
     "start_time": "2019-11-17T02:59:32.410162Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "tweets = api.user_timeline(tag, count=10)\n",
    "tweets_no_urls = [remove_url(tweet.text) for tweet in tweets]\n",
    "print(tweets_no_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:57:11.063110Z",
     "start_time": "2019-11-18T18:57:11.043114Z"
    }
   },
   "outputs": [],
   "source": [
    "# create data frame\n",
    "sa_df = pd.DataFrame(tweets1)\n",
    "sa_df.columns = ['sentence']\n",
    "print(sa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:57:13.503166Z",
     "start_time": "2019-11-18T18:57:13.483169Z"
    }
   },
   "outputs": [],
   "source": [
    "sa_df.insert(0, 'label', '', True)\n",
    "sa_df['label'] = 0\n",
    "print(sa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T18:57:20.256992Z",
     "start_time": "2019-11-18T18:57:20.240990Z"
    }
   },
   "outputs": [],
   "source": [
    "# sa_df.to_csv (r'C:\\Users\\zhfen\\Desktop\\Python\\AI1_hw\\tweet1.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Sentiments Using Twitter Data and Tweepy in Python\n",
    "https://www.earthdatascience.org/courses/earth-analytics-python/using-apis-natural-language-processing-twitter/analyze-tweet-sentiments-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T22:49:32.917403Z",
     "start_time": "2019-11-15T22:49:25.464507Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import tweepy as tw\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T22:49:34.090377Z",
     "start_time": "2019-11-15T22:49:33.848425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate to Twitter\n",
    "consumer_key = \"1KbEaA98iLFrLI68cWwZBojKa\"\n",
    "consumer_secret = \"UMuEh1JiSu8mVtOpWnDKatg2tXCEf3RGrOyUNBPfC0NgPgUVz0\"\n",
    "access_token = \"786958989347033088-k12TcKdTNLTcr16b9W2eAioz07eAwu1\"\n",
    "access_token_secret = \"3sU3lCoT6b6Y1Gf1DZPENCaYbbd5B18Dkp8iSYNuLWCSV\"\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Create API object\n",
    "api = tw.API(auth)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T22:49:37.041973Z",
     "start_time": "2019-11-15T22:49:37.035988Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T22:50:13.266958Z",
     "start_time": "2019-11-15T22:50:10.620746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom search term and define the number of tweets\n",
    "search_term = \"#climate+change -filter:retweets\"\n",
    "\n",
    "tweets = tw.Cursor(api.search,\n",
    "                   q=search_term,\n",
    "                   lang=\"en\",\n",
    "                   since='2019-11-01').items(100)\n",
    "\n",
    "# Remove URLs\n",
    "tweets_no_urls = [remove_url(tweet.text) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T22:50:14.137997Z",
     "start_time": "2019-11-15T22:50:14.128021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze Sentiments in Tweets\n",
    "sentiment_objects = [TextBlob(tweet) for tweet in tweets_no_urls]\n",
    "\n",
    "sentiment_objects[0].polarity, sentiment_objects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T22:50:15.005246Z",
     "start_time": "2019-11-15T22:50:14.957376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create list of polarity valuesx and tweet text\n",
    "sentiment_values = [[tweet.sentiment.polarity, str(tweet)] for tweet in sentiment_objects]\n",
    "\n",
    "sentiment_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T22:50:15.846889Z",
     "start_time": "2019-11-15T22:50:15.833924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframe containing the polarity value and tweet text\n",
    "sentiment_df = pd.DataFrame(sentiment_values, columns=[\"polarity\", \"tweet\"])\n",
    "\n",
    "# sentiment_df.head()\n",
    "print(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T22:50:18.026538Z",
     "start_time": "2019-11-15T22:50:17.571237Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot histogram of the polarity values\n",
    "sentiment_df.hist(bins=[-1, -0.75, -0.5, -0.25, 0.25, 0.5, 0.75, 1],\n",
    "             ax=ax,\n",
    "             color=\"purple\")\n",
    "\n",
    "plt.title(\"Sentiments from Tweets on Climate Change\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T22:50:21.124424Z",
     "start_time": "2019-11-15T22:50:20.553949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove polarity values equal to zero\n",
    "sentiment_df = sentiment_df[sentiment_df.polarity != 0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot histogram with break at zero\n",
    "sentiment_df.hist(bins=[-1, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1],\n",
    "             ax=ax,\n",
    "             color=\"purple\")\n",
    "\n",
    "plt.title(\"Sentiments from Tweets on Climate Change\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T17:59:00.492361Z",
     "start_time": "2019-12-04T17:57:39.269904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zhfen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From C:\\Users\\zhfen\\Desktop\\Python\\AI1_hw\\encoder.py:63: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "\n",
      "Search tag: @HarvardBiz\n",
      "\n",
      "0 RT MoHossain What Happens When Antitrust if any and Protectionism Cycles Collide HarvardBiz Monopoly Trader\n",
      "1 RT HarvardBiz To build something distinctive in the marketplace you first have to build something distinctive in the workplace\n",
      "0 RT MoHossain What Happens When Antitrust if any and Protectionism Cycles Collide HarvardBiz Monopoly Trader\n",
      "0 RT AmazingContent LeadershipLeading a group of people can produce chronic anxiety but many leaders who suffer from anxiety dont\n",
      "1 The best analysts are lightningfast coders who can surf vast datasets quickly encountering and surfacing potentia\n",
      "1 RT VALERIEAMALOU A6 Essential A plurality of voices and perspectives is what really helps you find new approaches at work Diversity is\n",
      "0 At the model companies innovation is a muscle that gets built up over the long haul It takes a lot of work befo\n",
      "1 A6 Essential A plurality of voices and perspectives is what really helps you find new approaches at work Diversi\n",
      "0 What Happens When Antitrust and Protectionism Cycles Collide HarvardBiz\n",
      "0 RT HarvardBiz Would you rather be happy in your life or happy about your life\n"
     ]
    }
   ],
   "source": [
    "%run SA_HuafengZhang.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
